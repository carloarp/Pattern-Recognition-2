# -*- coding: utf-8 -*-
"""distance metric learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1urzsLm_ldG_2vv2465mn3qHppLzq3P8U
"""

import numpy as np
import matplotlib as plt
import matplotlib.pyplot as plt
#import cv2
import sys
import time
import os
import psutil
import pandas as pd

from scipy.io import loadmat
from sklearn.neighbors import KNeighborsClassifier
from sklearn.decomposition import PCA
from scipy.linalg import eigh
from scipy.linalg import eig
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import normalize
from numpy.linalg import norm
from pandas import DataFrame
from mpl_toolkits.mplot3d import Axes3D
from numpy.linalg import matrix_rank
from numpy.linalg import inv

def describe_matrix(matrix,name):
	rank = matrix_rank(matrix)
	print(name,"has shape",matrix.shape,"and rank",rank,":\n",matrix,"\n")
	
def describe_list(list,name):
	length = len(list)
	print(name,"has length",len(list),":\n",list,"\n")

#load face data
def load_face_data(mat_file):
	## Unpacks the .mat file
	contents = loadmat(mat_file)			
	face_data = contents['X']
	face_label = contents['l']	
	return face_data,face_label
	

#partition data into training and test set
def partition_data(face_data,face_labels,show):
	## Dataset = 520 Images (52 classes, 10 images each)
	## Train Data = First 320 Images (class 1-32)
	## Test Data = Remaining 200 Images (class 33-52)

	x_train = face_data[:,0:320]
	x_test = face_data[:,320:520]
	
	y_train = face_labels[:,0:320].squeeze()
	y_test = face_labels[:,320:520].squeeze()
	
	if show == 'yes':
		describe_matrix(x_train,'Train Face Data (x_train)')
		describe_matrix(x_test,'Test Face Data (x_test)')
		describe_list(y_train,'Train Face Label (y_train)')
		describe_list(y_test,'Test Face Label (y_test)')
	
	return x_train,x_test,y_train,y_test

#normalised feature vectors into unit vectos in L2
def get_original_normalized_feature_vectors(X_train,show):
	original_train = X_train
	norm_train = normalize(X_train,axis=0,norm='l2')
	if show == 'yes':
		
		describe_matrix(original_train,'Original Train')
		describe_matrix(norm_train,'Norm Train')
	
	return original_train, norm_train

def euclid_dist_array(query_image, image_list):
    euclid_dist_array = []
    
    for i in range(0, image_list.shape[1]):
        query_image = query_image.astype(float)
        image = image_list[:,i].astype(float)
        
        subtract_elements = np.subtract(query_image,image)
        subtract_elements_squared = np.dot(subtract_elements, subtract_elements)
        sum = np.sum(subtract_elements_squared)
        euclid_dist = sum**0.5
        euclid_dist_array.append(np.round(euclid_dist, 3))
    
    return np.array(euclid_dist_array)

def average_precision(eval_table):
    precision_at_recall = []
    for recall_level in np.linspace(0.0, 1.0, 11):
        try:
            x_p = eval_table[eval_table['recall'] >= recall_level]['precision']
            interpolated_p = max(x_p)
        except:
            interpolated_p = 0.0
        precision_at_recall.append(interpolated_p)
    avg_prec = np.mean(precision_at_recall)
    
    return avg_prec

def get_df_score(rank_k, precision_list, recall_list):
    eval_table = pd.DataFrame()
    eval_table['@rank k'] = rank_k
    eval_table['precision'] = precision_list
    eval_table['recall'] = recall_list
    #AP = average_precision(eval_table)
    
    return eval_table

###the main
face_data,face_label = load_face_data('face.mat')

#### PARTITION DATA INTO TRAIN AND TEST SET
X_train,X_test,Y_train,Y_test = partition_data(face_data,face_label,show='no')

#### OBTAIN ORIGINAL AND NORMALIZED FEATURE VECTORS 
original_train, norm_train = get_original_normalized_feature_vectors(X_train,show='no')
original_test, norm_test = get_original_normalized_feature_vectors(X_test,show = 'no')


#### EXPRESS AS A DATAFRAME --- P1,P2,P3...P2576,CLASS
#df_original = get_df(original_train,Y_train,name='Original',show='no')
#df_norm = get_df(norm_train,Y_train,name='Norm',show='no')
#df_test = get_df(X_test,Y_test,name='Test',show='no')
#precision and recall with KNN
def k_NN(query_results, query_label, k):
    #retrieve top k results
    #define constants
    relevant = 0
    top_k = []

    #count number of relevant retrievals
    for i in range (0,k):
        top_k.append(query_results[i])
        if query_results[i] == query_label:
            relevant = relevant + 1

    precision = relevant/k 
    recall = relevant/9 
    
    return precision, recall

from sklearn.neighbors import NeighborhoodComponentsAnalysis
from sklearn.neighbors import KNeighborsClassifier
nca = NeighborhoodComponentsAnalysis(n_components = None, init = 'lda', max_iter = 1000, random_state = 42)

###apply to training data and transform to projected space

original_train_list = []
norm_train_list = []
for i in range(0,original_train.shape[1]):
    original_train_list.append(original_train[:,i])
for i in range(0,norm_train.shape[1]):
    norm_train_list.append(norm_train[:,i])
nca_original= nca.fit(original_train_list, Y_train)
nca_norm = nca.fit(norm_train_list, Y_train)

original_test_list = []
norm_test_list =[]
for i in range(0,original_test.shape[1]):
    original_test_list.append(original_test[:,i])
for i in range(0,norm_test.shape[1]):
    norm_test_list.append(norm_test[:,i])
nca_test = nca_original.transform(original_test_list)
nca_test_norm = nca_norm.transform(norm_test_list)
#try on nca_train
#nca_train = nca_original.transform(original_train_list)
#nca_train = nca_train.T
#print(nca_train.shape)
nca_test = nca_test.T
nca_test_norm = nca_test_norm.T
print(nca_test.shape)

rank_k = []
for i in range(1,nca_test.shape[1]):
    rank_k.append(i)
    
#initialise maP and accuracy scores
avg_prec = 0
rank1_prec = []
rank10_prec = []
for query_index in range(0,nca_test.shape[1]):
    query_image = nca_test_norm[:, query_index]
    image_list = np.delete(nca_test_norm, query_index, 1)
    query_label = Y_test[query_index]
    image_label_list = np.delete(Y_test, query_index, 0)

    euclid_dist = euclid_dist_array(query_image, image_list)

    #sort query from lowest to highest
    idx = euclid_dist.argsort()[::1]
    euclid_dist = euclid_dist[idx]
    query_results = image_label_list[idx]

    precision_list = []
    recall_list = []
    

    ##find accuracy scores for k nearest neighbours
    for k in range(1,nca_test.shape[1]):
        precision, recall = k_NN(query_results, query_label, k)
        precision_list.append(precision)
        recall_list.append(recall)

    rank1_prec.append(precision_list[0])
    rank10_prec.append(precision_list[9])


    eval_metric_table = get_df_score(rank_k, precision_list, recall_list)
    avg_prec = avg_prec + average_precision(eval_metric_table)
    #print(eval_metric_table)
    #title = 'total average precision up to image '+ str(query_index+1)
    #title = title + '= '
    #print(title, avg_prec)
    #print('\n')

maP = avg_prec/nca_test.shape[1]
rank1_prec = np.array(rank1_prec)
rank10_prec = np.array(rank10_prec)
#print(rank1_prec)
print(maP,'\n')
print('average rank 1 precision is ', np.mean(rank1_prec), '\n')
print('average rank 10 precision is', np.mean(rank10_prec),'\n')

!pip install pylmnn

"""From Wikipedia: Large margin nearest neighbors is an algorithm that learns this global (pseudo-)metric in a supervised fashion to improve the classification accuracy of the k-nearest neighbor rule.
Target neighbors are selected before learning. Each instance {\displaystyle {\vec {x}}_{i}}{\vec {x}}_{i} has exactly {\displaystyle k}k different target neighbors within {\displaystyle D}D, which all share the same class label {\displaystyle y_{i}}y_{i}. The target neighbors are the data points that should become nearest neighbors under the learned metric. Let us denote the set of target neighbors for a data point {\displaystyle {\vec {x}}_{i}}{\vec {x}}_{i} as {\displaystyle N_{i}}N_{i}.

Impostors
An impostor of a data point {\displaystyle {\vec {x}}_{i}}{\vec {x}}_{i} is another data point {\displaystyle {\vec {x}}_{j}}{\vec  x}_{j} with a different class label (i.e. {\displaystyle y_{i}\neq y_{j}}y_{i}\neq y_{j}) which is one of the nearest neighbors of {\displaystyle {\vec {x}}_{i}}{\vec {x}}_{i}. During learning the algorithm tries to minimize the number of impostors for all data instances in the training set

{\displaystyle \min _{\mathbf {M} }\sum _{i,j\in N_{i}}d({\vec {x}}_{i},{\vec {x}}_{j})+\lambda \sum _{i,j,l}\xi _{ijl}}
{\displaystyle \forall _{i,j\in N_{i},l,y_{l}\neq y_{i}}}\forall _{{i,j\in N_{i},l,y_{l}\neq y_{i}}}
{\displaystyle d({\vec {x}}_{i},{\vec {x}}_{j})+1-d({\vec {x}}_{i},{\vec {x}}_{l})\leq \xi _{ijl}}{\displaystyle d({\vec {x}}_{i},{\vec {x}}_{j})+1-d({\vec {x}}_{i},{\vec {x}}_{l})\leq \xi _{ijl}}
{\displaystyle \xi _{ijl}\geq 0}\xi _{{ijl}}\geq 0
{\displaystyle \mathbf {M} \succeq 0}{\mathbf  {M}}\succeq 0

For this coursework, PyLMNN package is used to compute LMNN for metric learning:https://pypi.org/project/PyLMNN/
"""

# need pip install pylmnn
from pylmnn import LargeMarginNearestNeighbor as LMNN
# Set up the hyperparameters
k_train, n_components, max_iter = 5, 25, 1000

# Instantiate the metric learner
lmnn = LMNN(n_neighbors=k_train, max_iter=max_iter, n_components= n_components)

# Train the metric learner
lmnn_original = lmnn.fit(original_train_list, Y_train)
lmnn_test = lmnn_original.transform(original_test_list)

lmnn_test= lmnn_test.T
print(lmnn_test.shape)

rank_k = []
for i in range(1,lmnn_test.shape[1]):
    rank_k.append(i)
    
#initialise maP and accuracy scores
avg_prec = 0
rank1_prec = []
rank10_prec = []
for query_index in range(0,lmnn_test.shape[1]):
    query_image = lmnn_test[:, query_index]
    image_list = np.delete(lmnn_test, query_index, 1)
    query_label = Y_test[query_index]
    image_label_list = np.delete(Y_test, query_index, 0)

    euclid_dist = euclid_dist_array(query_image, image_list)

    #sort query from lowest to highest
    idx = euclid_dist.argsort()[::1]
    euclid_dist = euclid_dist[idx]
    query_results = image_label_list[idx]

    precision_list = []
    recall_list = []
    

    ##find accuracy scores for k nearest neighbours
    for k in range(1,lmnn_test.shape[1]):
        precision, recall = k_NN(query_results, query_label, k)
        precision_list.append(precision)
        recall_list.append(recall)

    rank1_prec.append(precision_list[0])
    rank10_prec.append(precision_list[9])


    eval_metric_table = get_df_score(rank_k, precision_list, recall_list)
    avg_prec = avg_prec + average_precision(eval_metric_table)
    #print(eval_metric_table)
    #title = 'total average precision up to image '+ str(query_index+1)
    #title = title + '= '
    #print(title, avg_prec)
    #print('\n')

maP = avg_prec/lmnn_test.shape[1]
rank1_prec = np.array(rank1_prec)
rank10_prec = np.array(rank10_prec)
#print(rank1_prec)
print(maP,'\n')
print('average rank 1 precision is ', np.mean(rank1_prec), '\n')
print('average rank 10 precision is', np.mean(rank10_prec),'\n')

"""Linear Discriminant Projections, U projected"""

#LDP, compute C_D and C_S

#print(original_train.shape)
#print(original_test.shape)

#initialise C_D and C_S
C_D = np.zeros((2576, 2576))
C_S = np.zeros((2576, 2576))

K_d = 10
K_s = 8

for query_index in range(0, original_train.shape[1]):
  #this is the data to be queried
  query_image = original_train[:, query_index]
  image_list = np.delete(original_train, query_index, 1)
  query_label = Y_train[query_index]
  image_label_list = np.delete(Y_train, query_index, 0)

  euclid_dist = euclid_dist_array(query_image, image_list)

  #sort query from lowest to highest
  idx = euclid_dist.argsort()[::1]
  euclid_dist = euclid_dist[idx]
  query_results = image_label_list[idx]
  query_data = image_list[:,idx]
  #print(query_data.shape)

  count_kd = 0
  iteration = 0
  K_NN_diff = np.zeros((2576, K_d))
  K_NN_same = np.zeros((2576, K_s))
  #find k nearest non-similar points
  while count_kd < K_d:
    if query_results[iteration] != query_label:
      K_NN_diff[:, count_kd] = query_data[:, iteration]
      count_kd += 1
    iteration += 1
  
  count_ks = 0
  iteration = 0
#find k nearest similar points
  while count_ks < K_s:
    if query_results[iteration] == query_label:
      K_NN_same[:, count_ks] = query_data[:, iteration]
      count_ks += 1
    iteration += 1

  phi_d = np.subtract(K_NN_diff.T, query_image)
  phi_d = phi_d.T
  C_D += np.dot(phi_d, phi_d.T)

  phi_s = np.subtract(K_NN_same.T, query_image)
  phi_s = phi_s.T
  C_S += np.dot(phi_s, phi_s.T)

  #print(C_D.shape)

print(C_D.shape)
print(C_S.shape)

"""Notice that rank of C_S is singular, therefore use NCA to reduce dimensionality first"""

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
n_lda = 25
lda = LinearDiscriminantAnalysis(n_components= n_lda)
lda_original = lda.fit(original_train_list, Y_train)

#apply to training data
lda_train = lda_original.transform(original_train_list)
lda_train = lda_train.T
print(lda_train.shape)

# do the same thing as in [55] this time with a reduced projected space, nca_train
#initialise C_D and C_S

C_D = np.zeros((n_lde, n_lde))
C_S = np.zeros((n_lde, n_lde))

K_d = 5
K_s = 5

for query_index in range(0, lda_train.shape[1]):
  query_image = lda_train[:, query_index]
  image_list = np.delete(lda_train, query_index, 1)
  query_label = Y_train[query_index]
  image_label_list = np.delete(Y_train, query_index, 0)

  euclid_dist = euclid_dist_array(query_image, image_list)

  #sort query from lowest to highest
  idx = euclid_dist.argsort()[::1]
  euclid_dist = euclid_dist[idx]
  query_results = image_label_list[idx]
  query_data = image_list[:,idx]
  #print(query_data.shape)

  count_kd = 0
  iteration = 0
  K_NN_diff = np.zeros((n_lde, K_d))
  K_NN_same = np.zeros((n_lde, K_s))
  #find k nearest non-similar points
  while count_kd < K_d:
    if query_results[iteration] != query_label:
      K_NN_diff[:, count_kd] = query_data[:, iteration]
      #print(K_NN_diff.shape)
      count_kd += 1
    iteration += 1
  
  count_ks = 0
  iteration = 0
#find k nearest similar points
  while count_ks < K_s:
    if query_results[iteration] == query_label:
      K_NN_same[:, count_ks] = query_data[:, iteration]
      count_ks += 1
    iteration += 1

  phi_d = np.subtract(K_NN_diff.T, query_image)
  phi_d = phi_d.T
  C_D += np.dot(phi_d, phi_d.T)

  phi_s = np.subtract(K_NN_same.T, query_image)
  phi_s = phi_s.T
  C_S += np.dot(phi_s, phi_s.T)

print(C_D.shape)
print(C_S.shape)

eigenvalue, eigenvector = eig(np.dot(inv(C_S), C_D))

#sort eigenvector
idx = eigenvalue.argsort()[::-1]
eigenvalue = eigenvalue[idx]
eigenvector = eigenvector[:,idx]

#apply matrix to text data, first by the transform of LDA then linear projection
lda_test = lda_original.transform(original_test_list)
lda_test = lda_test.T
print(lda_test.shape)
#now the projection eigenvectors
lde_test = np.dot(eigenvector.T, lda_test)
print(lde_test.shape)

#evaluation metric
rank_k = []
for i in range(1,200):
    rank_k.append(i)
    
#initialise maP and accuracy scores
avg_prec = 0
rank1_prec = []
rank10_prec = []
for query_index in range(0,200):
    query_image = lde_test[:, query_index]
    image_list = np.delete(lde_test, query_index, 1)
    query_label = Y_test[query_index]
    image_label_list = np.delete(Y_test, query_index, 0)

    euclid_dist = euclid_dist_array(query_image, image_list)

    #sort query from lowest to highest
    idx = euclid_dist.argsort()[::1]
    euclid_dist = euclid_dist[idx]
    query_results = image_label_list[idx]

    precision_list = []
    recall_list = []
    

    ##find accuracy scores for k nearest neighbours
    for k in range(1,200):
        precision, recall = k_NN(query_results, query_label, k)
        precision_list.append(precision)
        recall_list.append(recall)

    rank1_prec.append(precision_list[0])
    rank10_prec.append(precision_list[9])

    eval_metric_table = get_df_score(rank_k, precision_list, recall_list)
    avg_prec = avg_prec + average_precision(eval_metric_table)
    #print(eval_metric_table)
    #title = 'total average precision up to image '+ str(query_index+1)
    #title = title + '= '
    #print(title, avg_prec)
    #print('\n')

maP = avg_prec/200
rank1_prec = np.array(rank1_prec)
rank10_prec = np.array(rank10_prec)
#print(rank1_prec)
print(maP,'\n')
print('average rank 1 precision is ', np.mean(rank1_prec), '\n')
print('average rank 10 precision is', np.mean(rank10_prec),'\n')

def plot_projected_3faces(X_test,mode, metric):
  if mode == 'show':
    c = ['red','blue','green','black']
    m = ['o','x','v','^']
    fig = plt.figure()
    ax = fig.add_subplot(111, projection='3d')
    
    count = 0

    for i in range(0,30):
      x = X_test[0,i]
      y = X_test[1,i]
      z = X_test[2,i]
      if (i%10) == 0:
        count = count+1
      ax.scatter(x, y, z, c=c[count], marker=m[count])
    ax.title.set_text(metric)
   
    plt.show()
    plt.close()

plot_projected_3faces(lde_test, mode = 'show', metric = 'LDE')
plot_projected_3faces(nca_test, mode = 'show', metric = 'NCA')
plot_projected_3faces(original_test, mode = 'show', metric = 'Baseline unnormed')
plot_projected_3faces(lmnn_test, mode = 'show', metric = 'LMNN')

plot_projected_3faces(lda_test, mode = 'show', metric = 'LDA')

